{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the python library gensim: https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heidi\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a premade word2vec model built on Google News articles.\n",
    "\n",
    "Download from: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the vocabulary to the 500,000 most common words.  Even at 500,000, it starts to get to nonsense words.  Here are the top 50 and bottom 50 words by frequency. And even for the real words that are infrequent, if a word is too obscure, it wouldn't make for a good clue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Most common:', [u'</s>', u'in', u'for', u'that', u'is', u'on', u'##', u'The', u'with', u'said', u'was', u'the', u'at', u'not', u'as', u'it', u'be', u'from', u'by', u'are', u'I', u'have', u'he', u'will', u'has', u'####', u'his', u'an', u'this', u'or', u'their', u'who', u'they', u'but', u'$', u'had', u'year', u'were', u'we', u'more', u'###', u'up', u'been', u'you', u'its', u'one', u'about', u'would', u'which', u'out'])\n",
      "('Least common:', [u'ideological_affinity', u'Rashtriya_Rifles_RR', u'Pedrotti', u'Frysinger', u'Ralph_Sacco', u'Ryan_Nece', u'Homs_Syria', u'BACK_TO_BACK', u'Nag_Hammadi', u'Dashan', u'Murape', u'Majolica', u'Sundvold', u'Jerryd', u'administered_subcutaneously', u'Pierre_Luc_Gagnon', u'Fedrizzi', u'CD_ROMS', u'Raynham_Mass.', u'NN_Vohra', u'Barraba', u'Delta_Upsilon', u'Roilo_Golez', u'Cindy_Scroggins', u'Iter', u'Ford_Expeditions', u'La_Toussuire', u'Hooksett_NH', u'ITCTransmission', u'wakeskate', u'Fervor', u'SAFT', u'steam_boiler', u'Moskwa', u'Inet_electronic', u'2A_1A', u'pituitary_tumor', u'Westernbank', u'3DV', u'Supremely', u'Mellars', u'JUDGMENT', u'thinnest_smartphone', u'BY_ROD_KLOECKNER', u'gamepads', u'Reventon', u'LongJump', u'Whitby_Dunlops', u'Chasing_Value', u'Pollution_Control_Agency'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Most common:\",model.index2word[:50])\n",
    "print(\"Least common:\",model.index2word[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example Codenames board.  `blue` is one team's words, `red` the other and `assassin` is the assassin word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board = {\n",
    "#     'blue': ['ambulance', 'hospital', 'spell', 'lock', 'charge', 'tail', 'link', 'cook', 'web'],\n",
    "#     'red': ['cat', 'button', 'pipe', 'pants', 'mount', 'sleep', 'stick', 'file', 'worm'],\n",
    "#     'assassin': ['doctor']\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use gensim to find the 10 words most related to \"ambulance\" in this word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = {\"blue\":[\"soul\",\"opera\",\"pin\",\"space\",\"battery\",\"tail\",\"draft\",\"grass\"],\"red\":[\"compound\",\"beat\",\"dragon\",\"spell\",\"face\",\"day\",\"Tokyo\",\"pan\",\"dance\"],\"assassin\":[\"kangaroo\"],\"neutral\":[\"luck\",\"straw\",\"glass\",\"ghost\",\"Saturn\",\"undertaker\",\"marble\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heidi\\Anaconda2\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'paramedics', 0.7590752243995667),\n",
       " (u'ambulances', 0.7493595480918884),\n",
       " (u'Ambulance', 0.7236292362213135),\n",
       " (u'paramedic', 0.662133514881134),\n",
       " (u'Ambulance_paramedics', 0.6315338611602783),\n",
       " (u'Ambulances', 0.6211477518081665),\n",
       " (u'LifeFlight_helicopter', 0.6147335171699524),\n",
       " (u'hospital', 0.6099206209182739),\n",
       " (u'Paramedics', 0.6081751585006714),\n",
       " (u'Ambulance_Service', 0.6080097556114197)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.similar_by_word('ambulance', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line is the word, followed by how similar the word is to \"ambulance.\" Some of these words word be useful, \"parametics\" for instance, but many are just other forms of the word \"ambulance.\"\n",
    "\n",
    "gensim allows us to directly find words the most similar to a whole group of words at one time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it produces a lot of nonsense words. We can use `restrict_vocab` to limit results to only the top n most common words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'For_Restrictions', 0.43488097190856934),\n",
       " (u'bed', 0.39588358998298645),\n",
       " (u'links', 0.38411831855773926),\n",
       " (u'hook', 0.38367366790771484),\n",
       " (u'paramedics', 0.38072746992111206),\n",
       " (u'emergency', 0.37950167059898376),\n",
       " (u'jail', 0.3759669065475464),\n",
       " (u'log', 0.37062549591064453),\n",
       " (u'intensive_care', 0.3661930561065674),\n",
       " (u'call', 0.36543411016464233),\n",
       " (u'webpage', 0.3649423122406006),\n",
       " (u'tow_truck', 0.3592333197593689),\n",
       " (u'click', 0.35906946659088135),\n",
       " (u'cooked', 0.3552851676940918),\n",
       " (u'care', 0.3537469208240509),\n",
       " (u'handcuff', 0.35027384757995605),\n",
       " (u'then', 0.34921103715896606),\n",
       " (u'stay', 0.3478427529335022),\n",
       " (u'turn', 0.34607696533203125),\n",
       " (u'bookmark', 0.3458564579486847)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\n",
    "    positive=board['blue'],\n",
    "    restrict_vocab=50000,\n",
    "    topn=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better, and produces some decent clues.  \n",
    "* \"bed\", \"paramedics\", \"emergency\" all relate to \"ambulance\" and \"hospital.\" \n",
    "* \"jail\" could relate to \"lock\" and \"charge.\" \n",
    "* \"click\" to \"web\" and \"link.\"\n",
    "\n",
    "But “bed” would also relate to the other team’s word “sleep”; and “click” with “button.” It would be bad to give clues which could point to the opponent’s cards. \n",
    "\n",
    "gensim allows for negative examples to be included as well to help avoid that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'easements', 0.18227766454219818),\n",
       " (u'easement', 0.17233158648014069),\n",
       " (u'unrestricted', 0.16916465759277344),\n",
       " (u'amplifier', 0.16338896751403809),\n",
       " (u'IEEE', 0.16030414402484894),\n",
       " (u'RF', 0.15936850011348724),\n",
       " (u'MHz', 0.15754832327365875),\n",
       " (u'Scouting', 0.15407733619213104),\n",
       " (u'Draft', 0.1501276195049286),\n",
       " (u'DoD', 0.1476529985666275)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg = board['red']\n",
    "neg += board['neutral']\n",
    "neg += board['assassin']\n",
    "\n",
    "model.most_similar(\n",
    "    positive=board['blue'],\n",
    "    negative=neg,\n",
    "    restrict_vocab=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really like the clue \"telemedicine.\" It's non-obvious, but relates to four words: \"web,\" \"link,\" \"ambulance\" and \"hospital.\" This shows the potential for this method to produce novel clues.\n",
    "\n",
    "Let's say that the clue were \"telemedicine\" and the four words were removed from the board, then the next team got a turn.  What might their clues be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'pillow', 0.43686941266059875),\n",
       " (u'bra', 0.3842337727546692),\n",
       " (u'couch', 0.38342970609664917),\n",
       " (u'tub', 0.37922778725624084),\n",
       " (u'closet', 0.36959999799728394),\n",
       " (u'sofa', 0.36713898181915283),\n",
       " (u'bathroom', 0.366258829832077),\n",
       " (u'bed', 0.36348700523376465),\n",
       " (u'crotch', 0.36245280504226685),\n",
       " (u'spoon', 0.36179912090301514)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = {\n",
    "    'blue': ['spell', 'lock', 'charge', 'tail', 'link'],\n",
    "    'red': ['cat', 'button', 'pipe', 'pants', 'mount', 'sleep', 'stick', 'file', 'worm'],\n",
    "    'assassin': 'doctor'\n",
    "}\n",
    "\n",
    "model.most_similar(\n",
    "    positive=board['red'],\n",
    "    negative=board['blue'],\n",
    "    restrict_vocab=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears much less successful.  The top words mostly just seem to relate to a singe word:\n",
    "* pillow -> sleep\n",
    "* bra -> pants\n",
    "* couch -> sleep? cat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Africa', 'agent', 'air', 'alien', 'alps', 'Amazon', 'ambulance', 'America', 'angel', 'Antarctica', 'apple', 'arm', 'Atlantis', 'Australia', 'Aztec', 'back', 'ball', 'band', 'bank', 'bar', 'bark', 'bat', 'battery', 'beach', 'bear', 'beat', 'bed', 'Beijing', 'bell', 'belt', 'Berlin', 'Bermuda', 'berry', 'bill', 'block', 'board', 'bolt', 'bomb', 'bond', 'boom', 'boot', 'bottle', 'bow', 'box', 'bridge', 'brush', 'buck', 'buffalo', 'bug', 'bugle', 'button', 'calf', 'Canada', 'cap', 'capital', 'car', 'card', 'carrot', 'casino', 'cast', 'cat', 'cell', 'centaur', 'center', 'chair', 'change', 'charge', 'check', 'chest', 'chick', 'China', 'chocolate', 'church', 'circle', 'cliff', 'cloak', 'club', 'code', 'cold', 'comic', 'compound', 'concert', 'conductor', 'contract', 'cook', 'copper', 'cotton', 'court', 'cover', 'crane', 'crash', 'cricket', 'cross', 'crown', 'cycle', 'Czech', 'dance', 'date', 'day', 'death', 'deck', 'degree', 'diamond', 'dice', 'dinosaur', 'disease', 'doctor', 'dog', 'draft', 'dragon', 'dress', 'drill', 'drop', 'duck', 'dwarf', 'eagle', 'Egypt', 'embassy', 'engine', 'England', 'Europe', 'eye', 'face', 'fair', 'fall', 'fan', 'fence', 'field', 'fighter', 'figure', 'file', 'film', 'fire', 'fish', 'flute', 'fly', 'foot', 'force', 'forest', 'fork', 'France', 'game', 'gas', 'genius', 'Germany', 'ghost', 'giant', 'glass', 'glove', 'gold', 'grace', 'grass', 'Greece', 'green', 'ground', 'ham', 'hand', 'hawk', 'head', 'heart', 'helicopter', 'himalayas', 'hole', 'Hollywood', 'honey', 'hood', 'hook', 'horn', 'horse', 'horseshoe', 'hospital', 'hotel', 'ice', 'ice', 'cream', 'India', 'iron', 'ivory', 'jack', 'jam', 'jet', 'Jupiter', 'kangaroo', 'ketchup', 'key', 'kid', 'king', 'kiwi', 'knife', 'knight', 'lab', 'lap', 'laser', 'lawyer', 'lead', 'lemon', 'leprechaun', 'life', 'light', 'limousine', 'line', 'link', 'lion', 'litter', 'Loch', 'Ness', 'lock', 'log', 'London', 'luck', 'mail', 'mammoth', 'maple', 'marble', 'march', 'mass', 'match', 'mercury', 'Mexico', 'microscope', 'millionaire', 'mine', 'mint', 'missile', 'model', 'mole', 'moon', 'Moscow', 'mount', 'mouse', 'mouth', 'mug', 'nail', 'needle', 'net', 'New', 'York', 'night', 'ninja', 'note', 'novel', 'nurse', 'nut', 'octopus', 'oil', 'olive', 'Olympus', 'opera', 'orange', 'organ', 'palm', 'pan', 'pants', 'paper', 'parachute', 'park', 'part', 'pass', 'paste', 'penguin', 'phoenix', 'piano', 'pie', 'pilot', 'pin', 'pipe', 'pirate', 'pistol', 'pit', 'pitch', 'plane', 'plastic', 'plate', 'platypus', 'play', 'plot', 'point', 'poison', 'pole', 'police', 'pool', 'port', 'post', 'pound', 'press', 'princess', 'pumpkin', 'pupil', 'pyramid', 'queen', 'rabbit', 'racket', 'ray', 'revolution', 'ring', 'robin', 'robot', 'rock', 'rome', 'root', 'rose', 'roulette', 'round', 'row', 'ruler', 'satellite', 'Saturn', 'scale', 'school', 'scientist', 'scorpion', 'screen', 'scuba', 'diver', 'seal', 'server', 'shadow', 'Shakespeare', 'shark', 'ship', 'shoe', 'shop', 'shot', 'sink', 'skyscraper', 'slip', 'slug', 'smuggler', 'snow', 'snowman', 'sock', 'soldier', 'soul', 'sound', 'space', 'spell', 'spider', 'spike', 'spine', 'spot', 'spring', 'spy', 'square', 'stadium', 'staff', 'star', 'state', 'stick', 'stock', 'straw', 'stream', 'strike', 'string', 'sub', 'suit', 'superhero', 'swing', 'switch', 'table', 'tablet', 'tag', 'tail', 'tap', 'teacher', 'telescope', 'temple', 'theater', 'thief', 'thumb', 'tick', 'tie', 'time', 'Tokyo', 'tooth', 'torch', 'tower', 'track', 'train', 'triangle', 'trip', 'trunk', 'tube', 'turkey', 'undertaker', 'unicorn', 'vacuum', 'van', 'vet', 'wake', 'wall', 'war', 'washer', 'Washington', 'watch', 'water', 'wave', 'web', 'well', 'whale', 'whip', 'wind', 'witch', 'worm', 'yard']\n"
     ]
    }
   ],
   "source": [
    "wordlist = open('../../assets/word_list.txt')\n",
    "words = wordlist.read().split()\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Africa', 'agent', 'air', 'alien', 'alps', 'Amazon', 'ambulance', 'America', 'angel', 'Antarctica', 'apple', 'arm', 'Atlantis', 'Australia', 'Aztec', 'back', 'ball', 'band', 'bank', 'bar', 'bark', 'bat', 'battery', 'beach', 'bear', 'beat', 'bed', 'Beijing', 'bell', 'belt', 'Berlin', 'Bermuda', 'berry', 'bill', 'block', 'board', 'bolt', 'bomb', 'bond', 'boom', 'boot', 'bottle', 'bow', 'box', 'bridge', 'brush', 'buck', 'buffalo', 'bug', 'bugle', 'button', 'calf', 'Canada', 'cap', 'capital', 'car', 'card', 'carrot', 'casino', 'cast', 'cat', 'cell', 'centaur', 'center', 'chair', 'change', 'charge', 'check', 'chest', 'chick', 'China', 'chocolate', 'church', 'circle', 'cliff', 'cloak', 'club', 'code', 'cold', 'comic', 'compound', 'concert', 'conductor', 'contract', 'cook', 'copper', 'cotton', 'court', 'cover', 'crane', 'crash', 'cricket', 'cross', 'crown', 'cycle', 'Czech', 'dance', 'date', 'day', 'death', 'deck', 'degree', 'diamond', 'dice', 'dinosaur', 'disease', 'doctor', 'dog', 'draft', 'dragon', 'dress', 'drill', 'drop', 'duck', 'dwarf', 'eagle', 'Egypt', 'embassy', 'engine', 'England', 'Europe', 'eye', 'face', 'fair', 'fall', 'fan', 'fence', 'field', 'fighter', 'figure', 'file', 'film', 'fire', 'fish', 'flute', 'fly', 'foot', 'force', 'forest', 'fork', 'France', 'game', 'gas', 'genius', 'Germany', 'ghost', 'giant', 'glass', 'glove', 'gold', 'grace', 'grass', 'Greece', 'green', 'ground', 'ham', 'hand', 'hawk', 'head', 'heart', 'helicopter', 'Himalayas', 'hole', 'Hollywood', 'honey', 'hood', 'hook', 'horn', 'horse', 'horseshoe', 'hospital', 'hotel', 'ice', 'ice_cream', 'India', 'iron', 'ivory', 'jack', 'jam', 'jet', 'Jupiter', 'kangaroo', 'ketchup', 'key', 'kid', 'king', 'kiwi', 'knife', 'knight', 'lab', 'lap', 'laser', 'lawyer', 'lead', 'lemon', 'leprechaun', 'Loch_Ness', 'life', 'light', 'limousine', 'line', 'link', 'lion', 'litter', 'lock', 'log', 'London', 'luck', 'mail', 'mammoth', 'maple', 'marble', 'march', 'mass', 'match', 'mercury', 'Mexico', 'microscope', 'millionaire', 'mine', 'mint', 'missile', 'model', 'mole', 'moon', 'Moscow', 'mount', 'mouse', 'mouth', 'mug', 'nail', 'needle', 'net', 'New_York', 'night', 'ninja', 'note', 'novel', 'nurse', 'nut', 'octopus', 'oil', 'olive', 'Olympus', 'opera', 'orange', 'organ', 'palm', 'pan', 'pants', 'paper', 'parachute', 'park', 'part', 'pass', 'paste', 'penguin', 'phoenix', 'piano', 'pie', 'pilot', 'pin', 'pipe', 'pirate', 'pistol', 'pit', 'pitch', 'plane', 'plastic', 'plate', 'platypus', 'play', 'plot', 'point', 'poison', 'pole', 'police', 'pool', 'port', 'post', 'pound', 'press', 'princess', 'pumpkin', 'pupil', 'pyramid', 'queen', 'rabbit', 'racket', 'ray', 'revolution', 'ring', 'robin', 'robot', 'rock', 'rome', 'root', 'rose', 'roulette', 'round', 'row', 'ruler', 'satellite', 'Saturn', 'scale', 'school', 'scientist', 'scorpion', 'screen', 'scuba_diver', 'seal', 'server', 'shadow', 'Shakespeare', 'shark', 'ship', 'shoe', 'shop', 'shot', 'sink', 'skyscraper', 'slip', 'slug', 'smuggler', 'snow', 'snowman', 'sock', 'soldier', 'soul', 'sound', 'space', 'spell', 'spider', 'spike', 'spine', 'spot', 'spring', 'spy', 'square', 'stadium', 'staff', 'star', 'state', 'stick', 'stock', 'straw', 'stream', 'strike', 'string', 'sub', 'suit', 'superhero', 'swing', 'switch', 'table', 'tablet', 'tag', 'tail', 'tap', 'teacher', 'telescope', 'temple', 'theater', 'thief', 'thumb', 'tick', 'tie', 'time', 'Tokyo', 'tooth', 'torch', 'tower', 'track', 'train', 'triangle', 'trip', 'trunk', 'tube', 'turkey', 'undertaker', 'unicorn', 'vacuum', 'van', 'vet', 'wake', 'wall', 'war', 'washer', 'Washington', 'watch', 'water', 'wave', 'web', 'well', 'whale', 'whip', 'wind', 'witch', 'worm', 'yard']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heidi\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "wordlist = open('../../assets/word_list.txt')\n",
    "words = wordlist.read().split(\"\\n\")\n",
    "print words\n",
    "for word in words:\n",
    "    if len(model.wv[word]) == 0:\n",
    "        print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../../assets/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'Word2VecKeyedVectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7e6874684245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../../assets/word2vec.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\heidi\\Anaconda2\\lib\\site-packages\\gensim\\models\\word2vec.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1321\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\heidi\\Anaconda2\\lib\\site-packages\\gensim\\models\\deprecated\\word2vec.pyc\u001b[0m in \u001b[0;36mload_old_word2vec\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mold_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vector_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     params = {\n",
      "\u001b[1;32mC:\\Users\\heidi\\Anaconda2\\lib\\site-packages\\gensim\\models\\deprecated\\word2vec.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1616\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1618\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1619\u001b[0m         \u001b[1;31m# update older models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1620\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\heidi\\Anaconda2\\lib\\site-packages\\gensim\\models\\deprecated\\old_saveload.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loaded %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\heidi\\Anaconda2\\lib\\site-packages\\gensim\\models\\deprecated\\old_saveload.pyc\u001b[0m in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'Word2VecKeyedVectors'"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(\"../../assets/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
